{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_preprocessing(data):\n",
    "    \"\"\"input: a list in which each element is a text/email\n",
    "        output: a list in which each text/email has beed processed i.e remove stopwords and numbers etc.\n",
    "    \"\"\"\n",
    "    for doc_id,text in enumerate(data):\n",
    "        '''Tokenization with a regex'''\n",
    "        tokens1 = tokenizer.tokenize(text.lower())\n",
    "        '''remove numbers'''\n",
    "        tokens2 = [token for token in tokens1 if(re.sub('\\d+','', token)!='')]\n",
    "        '''Stopword removal'''\n",
    "        filtered = [w for w in tokens2 if not w in stopwords.words('english')]\n",
    "        '''Lemmatization'''\n",
    "        filtered_lemm = []\n",
    "        for w in filtered:\n",
    "            noun = lemmatizer.lemmatize(w, pos='n')\n",
    "            verb = lemmatizer.lemmatize(w, pos='v')\n",
    "            if(noun!=w):\n",
    "                filtered_lemm.append(noun)\n",
    "            else:\n",
    "                filtered_lemm.append(verb)\n",
    "        '''Covenrt list of words to one string'''\n",
    "        doc = ' '.join(w for w in filtered_lemm) #giati to kaneis pali ena string...voleyei na einai lista apo tokens\n",
    "        data[doc_id] = doc  \n",
    "    return data\n",
    "\n",
    "class Textclassifier(object):\n",
    "    \"\"\"A class that will holds the train data, IDFs, and varius hyperparameters as feature vector size,  \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializer fucntion of the class\n",
    "        \"\"\"\n",
    "        selflIDF = {} #dictionary for the IDF values {(key-->word):(value--> IDF value)}\n",
    "        self.embeddings = None # words embeddings -- optional use\n",
    "        #variables about the data\n",
    "        self.train_raw_x = None\n",
    "        self.train_tokens_x = None\n",
    "        self.train_y = None\n",
    "        \n",
    "        self.test_raw_x = None\n",
    "        self.test_tokens_x = None\n",
    "        self.test_y = None\n",
    "        \n",
    "        #Set the type of tokenizer for the class (nltk.tokenizer)\n",
    "        self.tokenizer = RegexpTokenizer(r'\\b[^\\d\\W]+\\b') #This tokenizer removes punctuation and numbers\n",
    "        \n",
    "        #Set the type of lemmatizer for the class (nltk.stem.wordnet)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        \n",
    "    def load_data(self, train_dir,start,end):\n",
    "        \"\"\"input: a string which contains the path of training data and a 'start'  and an 'end' number of the folder name\n",
    "        i.e if we want to open from part1 to part9 folders then start=1 and end=10\n",
    "        output: one list in which each element is a text/email and another list in which each element is the target name of the \n",
    "        corresponding text(in our case 'spam' or 'ham')\n",
    "        \"\"\"\n",
    "        #keep path of files of training data\n",
    "        emails_path = [os.path.join(train_dir+ str(i),f) for i in range(start,end) for f in os.listdir(train_dir + str(i))] \n",
    "        emails = []\n",
    "        targets = []\n",
    "        i=0\n",
    "        for email_path in emails_path:\n",
    "            with open(email_path,encoding='utf-8') as f:\n",
    "                emails.append(f.readlines()[1:][1]) # we do not take the first line of each emails which is the subject\n",
    "                file_name = emails_path[i].split(\"\\\\\")[3]\n",
    "                if(file_name[0:5]==\"spmsg\"):\n",
    "                    targets.append('spam')\n",
    "                else:\n",
    "                    targets.append('ham')    \n",
    "            i+=1\n",
    "            \n",
    "        \"\"\"Get the  data and split it into train and test data\"\"\"\n",
    "        self.train_raw_x = emails[:int((len(emails)+1)*.80)] # 80% to training set\n",
    "        self.train_y = targets[:int((len(targets)+1)*.80)] # 80% to training set\n",
    "\n",
    "        self.test_raw_x = emails[int(len(emails)*.80+1):]#20% to test data\n",
    "        self.test_y = targets[int(len(targets) * .80+1):] #20% to test data\n",
    "    \n",
    "    def data_preprocessing(self, data):\n",
    "        \"\"\"input: a list in which each element is a text/email\n",
    "        output: a list in which each text/email has beed processed i.e remove stopwords and numbers etc.\n",
    "        \"\"\"\n",
    "        filtered_data = []\n",
    "        for text in data:\n",
    "            '''Tokenization with a regex'''\n",
    "            tokens1 = self.tokenizer.tokenize(text.lower())\n",
    "            '''Stopword removal'''\n",
    "            filtered = [w for w in tokens1 if not w in stopwords.words('english')]\n",
    "            '''Lemmatization'''\n",
    "            filtered_lemm = []\n",
    "            for w in filtered:\n",
    "                noun = self.lemmatizer.lemmatize(w, pos='n')\n",
    "                verb = self.lemmatizer.lemmatize(w, pos='v')\n",
    "                if(noun!=w):\n",
    "                    filtered_lemm.append(noun)\n",
    "                else:\n",
    "                    filtered_lemm.append(verb)\n",
    "            '''Covenrt list of words to one string'''\n",
    "            doc = ' '.join(w for w in filtered_lemm) #giati to kaneis pali ena string...voleyei na einai lista apo tokens\n",
    "            filtered_data.append(doc)  \n",
    "        return filtered_data\n",
    "    \n",
    "    def process(self,list_of_docs):\n",
    "        \"\"\"Takes a list of texts and apply the sklearn countvectorizer\"\"\"\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'date sun dec est michael mmorse yorku ca subject query wlodek zadrozny ask anything interest say construction np np second much relate might consider construction form discuss list late reduplication logical sense john mcnamara name tautologous thus level indistinguishable well well say john mcnamara name tautologous give support say logic base semantics irrelevant natural language sense tautologous supply value attribute follow attribute value fact value name attribute relevant entity chaim shmendrik john mcnamara name would false tautology reduplication either'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir =\".\\\\bare\\\\part\"\n",
    "'''load training data'''\n",
    "#emails,targets = load_data(train_dir,1,10)\n",
    "'''load test data'''\n",
    "#test_emails,test_targets = load_data(train_dir,10,11)\n",
    "my_classifier = Textclassifier()\n",
    "my_classifier.load_data(train_dir, 1,2)\n",
    "my_processed = my_classifier.data_preprocessing(my_classifier.train_raw_x)\n",
    "my_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'date sun dec est michael mmorse yorku ca subject query wlodek zadrozny ask anything interest say construction np np second much relate might consider construction form discuss list late reduplication logical sense john mcnamara name tautologous thus level indistinguishable well well say john mcnamara name tautologous give support say logic base semantics irrelevant natural language sense tautologous supply value attribute follow attribute value fact value name attribute relevant entity chaim shmendrik john mcnamara name would false tautology reduplication either'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'date sun dec est michael mmorse vm1 yorku ca subject query wlodek zadrozny ask anything interest say construction np np second much relate might consider construction form discuss list late reduplication logical sense john mcnamara name tautologous thus level indistinguishable well well say john mcnamara name tautologous give support say logic base semantics irrelevant natural language sense tautologous supply value attribute follow attribute value fact value name attribute relevant entity chaim shmendrik john mcnamara name would false tautology reduplication either'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''prepocessing test data'''\n",
    "test_data = data_preprocessing(test_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sposs sound pattern spontaneous speech production perception aix en provence france september european speech communication association esca identify area sound pattern spontaneous speech important area current research esca workshop organise aix en provence focus contribution describe explain spontaneous speech process perception word phrase sentence level wide variety language workshop theme last decade description spontaneous speech mainly focus reduction assimilation speech segment adjacent segment reduction assimilatory process spontaneous speech product gesture economy physical constraint also constrain phonetic phonological prosodic specificity language dialect therefore workshop aim contribute description understand production perception spontaneous speech process various language workshop centre around follow topic acoustic articulatory analysis spontaneous speech process prosodic information spontaneous speech process perception reduction assimilatory process model format workshop international workshop within limit number active participant e priority give person accept paper session introduce tutorial presentation invite expert paper present plenary session time demonstration discussion paper present poster session follow plenary discussion workshop site sposs take place conference centre locate area aix en provence ten minute drive aix en provence bus transportation centre provide every day detail logistic information distribute register participant proceeding language contribution workshop publish workshop proceeding available participant time workshop new french law loi toubon require include french abstract official language workshop english french registration fee fee workshop include proceeding lunch bus tranportation conference centre sposs reception ff ff reduction esca member student certificate status pay ff ff reduction esca student member registration non esca member include complementary membership important date march preliminary registration deadline submission title abstract may notification acceptance imstructions author information accomodation september imperative deadline early registration page camera ready paper september preliminary program e mail september worshop european speech communication association esca esca non profit organisation promote speech communication science technology european context limit number grant participation available information available though e mail esca icp inpg fr hrrp ophale icp inpg fr esca international scientific committee andrew butcher aust olle engstrand sw wolfgang hess ger klaus kohler ger florien koopmans van beinum ned bjorn lindblom sw joaquim llisterri sp francis nolan uk john ohala usa louis pol ned willy serniclaes bel jacqueline vaissiere fr organise committee danielle duez lpl bernard teston lpl marie helene casanova rossi lpl annie rival lpl martin brousseau lpl worshop secretariat correspondence concern workshop please use follow address sposs att danielle duez laboratoire parole et langage cnrs esa universite de provence avenue robert shuman aix en provence france phone fax e mail sposs lpl univ aix fr furteher information send preliminary registerd participant update information also available http www lpl univ aix fr'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
