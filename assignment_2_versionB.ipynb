{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 -- Text classification\n",
    "\n",
    "by Kolokathi Fotini (DS3516007)   and   Spartalis Iosif (DS3516018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA,TruncatedSVD\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "\n",
    " \n",
    "class LemmaTokenizer(object):\n",
    "    \"\"\"custom tokenizer with lemmatizer to be used in sklearn countVectorizer\"\"\"\n",
    "    def __init__(self):\n",
    "        self.tkzer = RegexpTokenizer(r'\\b[^\\d\\W]+\\b') #This tokenizer removes punctuation and numbers\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        filtered_lemm = []\n",
    "        for w in self.tkzer.tokenize(doc):\n",
    "            noun = self.wnl.lemmatize(w, pos='n')\n",
    "            verb = self.wnl.lemmatize(w, pos='v')\n",
    "            if(noun!=w):\n",
    "                filtered_lemm.append(noun)\n",
    "            else:\n",
    "                filtered_lemm.append(verb)\n",
    "            \n",
    "        return filtered_lemm \n",
    "\n",
    "'''\n",
    "input: a string which contains the path of  data and a 'start'  and an 'end' number of the folder name\n",
    "i.e if we want to open from part1 to part9 folders then start=1 and end=10\n",
    "output: one list in which each element is a text/email and another list in which each element is the target name of the \n",
    "corresponding text(in our case 'spam' or 'ham')\n",
    "'''\n",
    "def load_data(train_dir,start,end):\n",
    "    \"keep path of files of training data\"\n",
    "    emails_path = [os.path.join(train_dir+ str(i),f) for i in range(start,end) for f in os.listdir(train_dir + str(i))] \n",
    "    emails = []\n",
    "    targets = []\n",
    "    i=0\n",
    "    for email_path in emails_path:\n",
    "        with open(email_path,encoding='utf-8') as f:\n",
    "            emails.append(f.readlines()[1:][1]) # we do not take the first line of each emails which is the subject\n",
    "            file_name = emails_path[i].split(\"\\\\\")[3]\n",
    "            if(file_name[0:5]==\"spmsg\"):\n",
    "                targets.append('spam')\n",
    "            else:\n",
    "                targets.append('ham')    \n",
    "        i+=1\n",
    "    return emails,targets\n",
    "\n",
    "'''\n",
    "input: a list in which each element is a text/email\n",
    "output: a list in which each text/email has beed processed i.e remove stopwords and numbers etc.\n",
    "'''\n",
    "def data_preprocessing(data):\n",
    "    for doc_id,text in enumerate(data):\n",
    "        '''Tokenization with a regex'''\n",
    "        tokens1 = tokenizer.tokenize(text.lower())\n",
    "        '''remove numbers'''\n",
    "        tokens2 = [token for token in tokens1 if(re.sub('\\d+','', token)!='')]\n",
    "        '''Stopword removal'''\n",
    "        filtered = [w for w in tokens2 if not w in stopwords.words('english')]\n",
    "        '''Lemmatization'''\n",
    "        filtered_lemm = []\n",
    "        for w in filtered:\n",
    "            noun = lemmatizer.lemmatize(w, pos='n')\n",
    "            verb = lemmatizer.lemmatize(w, pos='v')\n",
    "            if(noun!=w):\n",
    "                filtered_lemm.append(noun)\n",
    "            else:\n",
    "                filtered_lemm.append(verb)\n",
    "        '''Covenrt list of words to one string'''\n",
    "        doc = ' '.join(w for w in filtered_lemm)\n",
    "        data[doc_id] = doc  \n",
    "    return data\n",
    "\n",
    "\n",
    "'''\n",
    "unput : \n",
    "    tfs: doc-term numpy array in which each element contains the corresponding tf(t,d)\n",
    "output:\n",
    "    a tf-idf numpy array with normalized term vectors\n",
    "'''\n",
    "def tf_idf(tfs):\n",
    "    idf_list = []\n",
    "    # total number of docs\n",
    "    N = tfs.shape[0] \n",
    "    # total number of terms\n",
    "    terms = tfs.shape[1] \n",
    "    '''Here we calculate the idf for each term of doc-term matrix and append each of it in a list'''\n",
    "    for i in range (0,terms):\n",
    "        '''\n",
    "        see: \n",
    "        http://stackoverflow.com/questions/36966019/how-aretf-idf-calculated-by-the-scikit-learn-tfidfvectorizer\n",
    "        http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer    \n",
    "        http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "        idf(d, t) = log [ (1 + N) / (1 + df(d, t)) ] + 1.\n",
    "        '''\n",
    "        idf_list.append((np.log((N+1)/(np.count_nonzero(tfs[:,i])+1))+1))\n",
    "\n",
    "    '''Here we create a numpy array in which we have concatenated the idf_list N times'''\n",
    "    idf_array = np.array(idf_list) # convert idf_list to numpy array\n",
    "    idfs = np.tile(idf_array,(N,1))\n",
    "    ''' tf-idf(d, t) = tf(d,t) * idf(d, t)'''\n",
    "    tf_idf = tfs*idfs\n",
    "    ''' Normalize term vectors of tf_idf array with l2 norm'''\n",
    "    # see: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize\n",
    "    tf_idf_normalized = normalize(tf_idf,axis=0)\n",
    "    return tf_idf_normalized\n",
    "\n",
    "'''\n",
    "input:\n",
    "    train_X: tf-idf array of training data\n",
    "    test_X: tf-idf array of test data\n",
    "    k: number of most important principal components we will keep after dimentionality reduction\n",
    "output:\n",
    "    train_X and test_X transformed numpy arrays\n",
    "'''\n",
    "def select_features_pca(train_X, test_X, k=1000):\n",
    "    selector = PCA(n_components=k)\n",
    "    selector.fit(train_X)\n",
    "    train_X = selector.transform(train_X)\n",
    "    test_X = selector.transform(test_X)\n",
    "    return train_X, test_X\n",
    "\n",
    "'''\n",
    "In practice TruncatedSVD is useful on large sparse datasets which cannot be centered without making the memory usage \n",
    "explode.\n",
    "see:\n",
    "http://scikit-learn.org/stable/modules/decomposition.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "\n",
    "input:\n",
    "    train_X: tf-idf array of training data\n",
    "    test_X: tf-idf array of test data\n",
    "    k: number of most important principal components we will keep after dimentionality reduction\n",
    "output:\n",
    "    train_X and test_X transformed numpy arrays\n",
    "'''\n",
    "def select_features_svd(train_X, test_X, k=1000):\n",
    "    selector = TruncatedSVD(n_components=k)\n",
    "    selector.fit(train_X)\n",
    "    train_X = selector.transform(train_X)\n",
    "    test_X = selector.transform(test_X)\n",
    "    return train_X, test_X\n",
    "\n",
    "'''\n",
    "input:\n",
    "   test_targets: list with the test targets \n",
    "   y_pred: list with the predicted test targets\n",
    "'''\n",
    "def report(test_targets,y_pred):\n",
    "    '''prints classification report and confusion matrix'''\n",
    "    target_names = ['spam','ham']\n",
    "    print('Accuracy: %s' % metrics.accuracy_score(test_targets,y_pred))\n",
    "    print()\n",
    "    '''Classification report'''\n",
    "    print('------------------------Classification report------------------------')\n",
    "    print()\n",
    "    print(classification_report(test_targets, y_pred, target_names=target_names))\n",
    "    print('---------------------------------------------------------------------')\n",
    "    print('Confusion matrix:')\n",
    "    print(metrics.confusion_matrix(test_targets,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dir =\".\\\\bare\\\\part\"\n",
    "\n",
    "'''load data'''\n",
    "emails,targets = load_data(train_dir,1,11)\n",
    "\n",
    "'''shuffle data'''\n",
    "c = list(zip(emails, targets))\n",
    "random.shuffle(c)\n",
    "emails, targets = zip(*c)\n",
    "emails = list(emails)\n",
    "targets = list(targets)\n",
    "\n",
    "'''split targets for training and test data'''\n",
    "train_targets = targets[:int((len(targets)+1)*.80)] # 80% to training set\n",
    "test_targets = targets[int(len(targets) * .80+1):] # 20% to test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer(r'\\w+') #This tokenizer removes punctuation\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "'''prepocessing of data'''\n",
    "#data = data_preprocessing(emails)\n"
    data  = emails
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ##  Tf-idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''document-term matrix with Tfs'''\n",
    "vectorizer = CountVectorizer(min_df=1,tokenizer=LemmaTokenizer(), stop_words = 'english')\n",
    "tfs = vectorizer.fit_transform(data)\n",
    "tfs = tfs.toarray() # convert matrix to numpy array\n",
    "\n",
    "'''tf-idf document-term numpy array'''\n",
    "tf_idf_ = tf_idf(tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893, 49347)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dimentionality reduction\n",
    "\n",
    "#'''PCA'''\n",
    "#train_data_pca,test_data_pca = select_features_pca(tf_idf_[:int((tf_idf_.shape[0]+1)*.80)],tf_idf_[int(tf_idf_.shape[0]*.80+1):])\n",
    "'''SVD'''\n",
    "train_data_svd,test_data_svd = select_features_svd(tf_idf_[:int((tf_idf_.shape[0]+1)*.80)],tf_idf_[int(tf_idf_.shape[0]*.80+1):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Execute Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137.79602575302124\n"
     ]
    }
   ],
   "source": [
    "# ~2 mins to run\n",
    "\n",
    "# Import Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Here C=1/lambda.So larger C ==> weaker regularization(If we want to prevent underfitting)\n",
    "# Smallar C ==> stronger regularization (If we want to prevent overfitting)\n",
    "param_grid = {'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "              'fit_intercept':[True,False],'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag'],\n",
    "             'tol':[1e-2,1e-3,1e-4,1e-5]}\n",
    "\n",
    "# Hyperparameter cv=3  by default, so we make 3-fold cross validation.If we set cv=10 it takes more time to run\n",
    "gs_clf = GridSearchCV(LogisticRegression(), param_grid,n_jobs=-1)\n",
    "\n",
    "'''train classifier'''\n",
    "gs_clf = gs_clf.fit(train_data_svd,train_targets)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=False,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='newton-cg', tol=0.01,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Estimator values.It has default parameters that we did not specify'''\n",
    "gs_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'fit_intercept': False, 'solver': 'newton-cg', 'tol': 0.01}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Get the best parameter values directrly'''\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.980968858131\n",
      "\n",
      "------------------------Classification report------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       spam       0.98      1.00      0.99       479\n",
      "        ham       1.00      0.89      0.94        99\n",
      "\n",
      "avg / total       0.98      0.98      0.98       578\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Confusion matrix:\n",
      "[[479   0]\n",
      " [ 11  88]]\n"
     ]
    }
   ],
   "source": [
    "'''make prediction'''\n",
    "y_pred = gs_clf.predict(test_data_svd)\n",
    "\n",
    "'''Classification report'''\n",
    "report(test_targets,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.980968858131\n",
      "\n",
      "------------------------Classification report------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       spam       0.98      1.00      0.99       479\n",
      "        ham       1.00      0.89      0.94        99\n",
      "\n",
      "avg / total       0.98      0.98      0.98       578\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Confusion matrix:\n",
      "[[479   0]\n",
      " [ 11  88]]\n"
     ]
    }
   ],
   "source": [
    "'''Using the best parameters to make predictions with logistic regression'''\n",
    "\n",
    "''' Load classifier '''\n",
    "LR = LogisticRegression(C=gs_clf.best_params_['C'],fit_intercept=gs_clf.best_params_['fit_intercept'],\n",
    "                        solver=gs_clf.best_params_['solver'],tol=gs_clf.best_params_['tol'])\n",
    "\n",
    "'''Train classifier''' \n",
    "regr_clf = LR.fit(train_data_svd,train_targets)\n",
    "\n",
    "'''make prediction'''\n",
    "Y_pred_regr = regr_clf.predict(test_data_svd)\n",
    "\n",
    "'''Classification report''' \n",
    "report(test_targets,Y_pred_regr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.968662977218628\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# define the parameter values that should be searched\n",
    "k_range = list(range(1, 31))\n",
    "\n",
    "# Another parameter besides k that we might vary is the weights parameters\n",
    "# uniform (all points in the neighborhood are weighted equally)\n",
    "# distance (weights closer neighbors more heavily than further neighbors)\n",
    "\n",
    "param_grid = {'n_neighbors':k_range,'weights':['uniform','distance'],'algorithm':['auto','ball_tree','kd_tree','brute'],\n",
    "             'metric':['euclidean','cityblock','minkowski'],'p':[1,2]}\n",
    "\n",
    "gs_clf = RandomizedSearchCV(KNeighborsClassifier(),param_grid,n_jobs=-1)\n",
    "\n",
    "'''train classifier'''\n",
    "gs_clf = gs_clf.fit(train_data_svd,train_targets)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='ball_tree', leaf_size=30, metric='euclidean',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=21, p=1,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Estimator values.It has default parameters that we did not specify'''\n",
    "gs_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'ball_tree',\n",
       " 'metric': 'euclidean',\n",
       " 'n_neighbors': 21,\n",
       " 'p': 1,\n",
       " 'weights': 'distance'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Get the best parameter values directrly'''\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93598615917\n",
      "\n",
      "------------------------Classification report------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       spam       0.95      0.98      0.96       479\n",
      "        ham       0.88      0.73      0.80        99\n",
      "\n",
      "avg / total       0.93      0.94      0.93       578\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Confusion matrix:\n",
      "[[469  10]\n",
      " [ 27  72]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "'''make prediction'''\n",
    "y_pred = gs_clf.predict(test_data_svd)\n",
    "\n",
    "''' Classification report'''\n",
    "report(test_targets,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93598615917\n",
      "\n",
      "------------------------Classification report------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       spam       0.95      0.98      0.96       479\n",
      "        ham       0.88      0.73      0.80        99\n",
      "\n",
      "avg / total       0.93      0.94      0.93       578\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Confusion matrix:\n",
      "[[469  10]\n",
      " [ 27  72]]\n"
     ]
    }
   ],
   "source": [
    "'''Using the best parameters to make predictions with KNN'''\n",
    "\n",
    "\n",
    "''' Load classifier '''\n",
    "KNN = KNeighborsClassifier(n_neighbors=gs_clf.best_params_['n_neighbors'],weights=gs_clf.best_params_['weights'],algorithm=gs_clf.best_params_['algorithm'],\n",
    "                           metric=gs_clf.best_params_['metric'],p=gs_clf.best_params_['p'])\n",
    "\n",
    "'''Train classifier''' \n",
    "knn_clf = KNN.fit(train_data_svd,train_targets)\n",
    "\n",
    "'''make prediction'''\n",
    "Y_pred_knn = knn_clf.predict(test_data_svd)\n",
    "\n",
    "'''Classification report''' \n",
    "report(test_targets,Y_pred_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* #### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.52198362350464\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import scipy.stats\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# For hyperparameter C see:\n",
    "# https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel\n",
    "\n",
    "#param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "#              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], 'kernel':['linear','poly','rbf','sigmoid']}\n",
    "\n",
    "#param_grid = {'C': np.logspace(-3, 2, 6),\n",
    "#              'gamma': np.logspace(-3, 2, 6), 'kernel':['linear','poly','rbf','sigmoid']}\n",
    "\n",
    "param_grid = {'C': scipy.stats.expon(scale=100),\n",
    "              'gamma': scipy.stats.expon(scale=.1), 'kernel':['linear','poly','rbf','sigmoid']}\n",
    "\n",
    "\n",
    "gs_clf = RandomizedSearchCV(SVC(), param_grid,n_jobs=-1)\n",
    "\n",
    "'''train classifier'''\n",
    "gs_clf = gs_clf.fit(train_data_svd,train_targets)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=165.33685710660166, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=0.19885151049491478,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Estimator values.It has default parameters that we did not specify'''\n",
    "gs_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 165.33685710660166, 'gamma': 0.19885151049491478, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Get the best parameter values directrly'''\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.979238754325\n",
      "\n",
      "------------------------Classification report------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       spam       0.98      0.99      0.99       479\n",
      "        ham       0.97      0.91      0.94        99\n",
      "\n",
      "avg / total       0.98      0.98      0.98       578\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Confusion matrix:\n",
      "[[476   3]\n",
      " [  9  90]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "'''make prediction'''\n",
    "y_pred = gs_clf.predict(test_data_svd)\n",
    "\n",
    "''' Classification report'''\n",
    "report(test_targets,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.979238754325\n",
      "\n",
      "------------------------Classification report------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       spam       0.98      0.99      0.99       479\n",
      "        ham       0.97      0.91      0.94        99\n",
      "\n",
      "avg / total       0.98      0.98      0.98       578\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Confusion matrix:\n",
      "[[476   3]\n",
      " [  9  90]]\n"
     ]
    }
   ],
   "source": [
    "'''Using the best parameters to make predictions with SVM'''\n",
    "\n",
    "\n",
    "''' Load classifier '''\n",
    "SVM = SVC(C=gs_clf.best_params_['C'],gamma=gs_clf.best_params_['gamma'],kernel=gs_clf.best_params_['kernel'])\n",
    "\n",
    "'''Train classifier''' \n",
    "svm_clf = SVM.fit(train_data_svd,train_targets)\n",
    "\n",
    "'''make prediction'''\n",
    "Y_pred_svm = svm_clf.predict(test_data_svd)\n",
    "\n",
    "'''Classification report''' \n",
    "report(test_targets,Y_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For MultinomialNB you can see: http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "\n",
    "'''\n",
    "There are 3 versions of naive bayes algorithm as we can see from here: http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "1.GaussianNB which is not a good fit for document classification.\n",
    "2.BernoulliNB which is good for document classification but it accepts inly boolean features.\n",
    "3.MultinomialNB is good for document classification.It is suitable for classification with discrete features (e.g., word counts \n",
    "for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, \n",
    "fractional counts such as tf-idf also work well.\n",
    "\n",
    "As SVD may return negative elements, we cannot use MultinomialNB.So we will take tf-idf numpy array without making dimentionality\n",
    "reduction.Of course we do not wait as good results as the previous algorithms gave.\n",
    "'''\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'fit_prior':[True,False]}\n",
    "gs_clf = GridSearchCV(MultinomialNB(), param_grid)\n",
    "\n",
    "'''train classifier'''\n",
    "gs_clf = gs_clf.fit(tf_idf_[:int((tf_idf_.shape[0]+1)*.80)],train_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Estimator values.It has default parameters that we did not specify'''\n",
    "gs_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_prior': True}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Get the best parameter values directrly'''\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.787197231834\n",
      "\n",
      "------------------------Classification report------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       spam       0.97      0.77      0.86       479\n",
      "        ham       0.44      0.88      0.59        99\n",
      "\n",
      "avg / total       0.88      0.79      0.81       578\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Confusion matrix:\n",
      "[[368 111]\n",
      " [ 12  87]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "'''make prediction'''\n",
    "y_pred = gs_clf.predict(tf_idf_[int(tf_idf_.shape[0]*.80+1):])\n",
    "\n",
    "''' Classification report'''\n",
    "report(test_targets,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.787197231834\n",
      "\n",
      "------------------------Classification report------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       spam       0.97      0.77      0.86       479\n",
      "        ham       0.44      0.88      0.59        99\n",
      "\n",
      "avg / total       0.88      0.79      0.81       578\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Confusion matrix:\n",
      "[[368 111]\n",
      " [ 12  87]]\n"
     ]
    }
   ],
   "source": [
    "'''Using the best parameters to make predictions with Naive Bayes'''\n",
    "\n",
    "\n",
    "''' Load classifier '''\n",
    "NaiveBayes = MultinomialNB(fit_prior=gs_clf.best_params_['fit_prior'])\n",
    "\n",
    "'''Train classifier''' \n",
    "nb_clf = NaiveBayes.fit(tf_idf_[:int((tf_idf_.shape[0]+1)*.80)],train_targets)\n",
    "\n",
    "'''make prediction'''\n",
    "Y_pred_nb = nb_clf.predict(tf_idf_[int(tf_idf_.shape[0]*.80+1):])\n",
    "\n",
    "'''Classification report''' \n",
    "report(test_targets,Y_pred_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* ## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "'''\n",
    "output:\n",
    "    model: a dictionary which has as key the word and as value the numpy array with one row the corresponding vector\n",
    "'''\n",
    "def load_embeddings(gloveFile):\n",
    "    f = open(gloveFile,'r',encoding='utf8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        word = line.split(' ')[0]\n",
    "        model[word] = np.array(line.split(' ')[1:],dtype='f')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings = load_embeddings('glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n#ARGEI POLY...............\\nstart_time = time.time()\\n\\n# data: processed data\\nT = np.zeros((len(data),300))\\n\\nfor doc_id, text in enumerate(data):\\n    word_sequences=word_tokenize(text)\\n    c =0 \\n    for word in word_sequences:\\n        if word in list(embeddings):\\n            c+=1\\n            T[doc_id,:]+=embeddings[word]\\n    if(c>0):\\n        T[doc_id,:] = T[doc_id,:]/c\\n            \\n       \\nelapsed_time = time.time() - start_time\\nprint(elapsed_time)\\n\\n\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "#ARGEI POLY...............\n",
    "start_time = time.time()\n",
    "\n",
    "# data: processed data\n",
    "T = np.zeros((len(data),300))\n",
    "\n",
    "for doc_id, text in enumerate(data):\n",
    "    word_sequences=word_tokenize(text)\n",
    "    c =0 \n",
    "    for word in word_sequences:\n",
    "        if word in list(embeddings):\n",
    "            c+=1\n",
    "            T[doc_id,:]+=embeddings[word]\n",
    "    if(c>0):\n",
    "        T[doc_id,:] = T[doc_id,:]/c\n",
    "            \n",
    "       \n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
